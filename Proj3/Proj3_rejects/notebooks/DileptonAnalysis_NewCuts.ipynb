{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When running this notebook via the Galaxy portal\n",
    "You can access your data via the dataset number. Using a Python kernel, you can access dataset number 42 with ``handle = open(get(42), 'r')``.\n",
    "To save data, write your data to a file, and then call ``put('filename.txt')``. The dataset will then be available in your galaxy history.\n",
    "<br><br>Note that if you are putting/getting to/from a different history than your default history, you must also provide the history-id.\n",
    "<br><br>More information including available galaxy-related environment variables can be found at https://github.com/bgruening/docker-jupyter-notebook. This notebook is running in a docker container based on the Docker Jupyter container described in that link.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dilepton analysis  (Python) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of a simple dilepton analysis, quite similar to the notebook called \"Dilepton_analysis_noData.ipynb\", but with some differences. The most obvious difference is that we here also include real data. This example also has a slightly more advanced event selection.  \n",
    "\n",
    "**Notice:** This is *only an example* on how to do this. Feel free to be creative, and to find better and/or more elegant ways of doing the various steps! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.24/02\n",
      "importing Jupyter notebook from setPath.ipynb\n",
      "importing Jupyter notebook from /storage/galaxy/jobs_directory/003/3037/working/jupyter/Input/OpenDataPandaFramework13TeV.ipynb\n",
      "This library contains handy functions to ease the access and use of the 13TeV ATLAS OpenData release\n",
      "\n",
      "getBkgCategories()\n",
      "\t Dumps the name of the various background cataegories available \n",
      "\t as well as the number of samples contained in each category.\n",
      "\t Returns a vector with the name of the categories\n",
      "\n",
      "getSamplesInCategory(cat)\n",
      "\t Dumps the name of the samples contained in a given category (cat)\n",
      "\t Returns dictionary with keys being DSIDs and values physics process name from filename.\n",
      "\n",
      "getMCCategory()\n",
      "\t Returns dictionary with keys DSID and values MC category\n",
      "\n",
      "initialize(indir)\n",
      "\t Collects all the root files available in a certain directory (indir)\n",
      "\n",
      "\n",
      "\n",
      "Setting luminosity to 10064 pb^-1\n",
      "\n",
      "###############################\n",
      "#### Background categories ####\n",
      "###############################\n",
      "Category             N(samples)\n",
      "-------------------------------\n",
      "Diboson                      10\n",
      "Higgs                        20\n",
      "Wjets                        42\n",
      "Wjetsincl                     6\n",
      "Zjets                        42\n",
      "Zjetsincl                     3\n",
      "singleTop                     6\n",
      "topX                          3\n",
      "ttbar                         1\n",
      "###############################\n",
      "#### Signal categories ####\n",
      "###############################\n",
      "Category             N(samples)\n",
      "-------------------------------\n",
      "GG_ttn1                       4\n",
      "Gee                           5\n",
      "Gmumu                         5\n",
      "RS_G_ZZ                       5\n",
      "SUSYC1C1                     10\n",
      "SUSYC1N2                     18\n",
      "SUSYSlepSlep                 14\n",
      "TT_directTT                   4\n",
      "ZPrimeee                      4\n",
      "ZPrimemumu                    4\n",
      "ZPrimett                     12\n",
      "dmV_Zll                      10\n"
     ]
    }
   ],
   "source": [
    "import ROOT as R\n",
    "import import_ipynb\n",
    "import setPath\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from Input.OpenDataPandaFramework13TeV import *\n",
    "%jsroot on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the analaysis to run (*1largeRjet1lep*, *1lep1tau*, *3lep*, *exactly2lep*, *GamGam*, *2lep*, *4lep*)\n",
    "\n",
    "Set the directory where you have downloaded the ATLAS OpenData samples you want to run over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "opendatadir = \"/storage/shared/data/fys5555/ATLAS_opendata/\"\n",
    "analysis = \"2lep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = R.TChain(\"mini\")\n",
    "data = R.TChain(\"mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of all the background samples, category and their IDs can be found in **Infofile.txt**. The cross-section, efficiencies etc. needed for scaling are stored in the **Files_<---->**. We read these files and add all the samples to the TChain. We also (for later convenience) make a vector containing the dataset IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING \t File topX.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File Zjetsincl.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File Diboson.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File Gmumu.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File ZPrimeee.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File dmV_Zll.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File RS_G_ZZ.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File Higgs.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File ZPrimemumu.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File ZPrimett.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File Wjetsincl.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File Gee.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File Zjets.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File Wjets.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File SUSYC1C1.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File GG_ttn1.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File TT_directTT.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File SUSYC1N2.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File SUSYSlepSlep.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File ttbar.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File singleTop.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "####################################################################################################\n",
      "BACKGROUND SAMPLES\n",
      "####################################################################################################\n",
      "WARNING \t File for ggH125_tautaulh not found in /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC\n",
      "WARNING \t File for VBFH125_tautaulh not found in /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC\n",
      "####################################################################################################\n",
      "SIGNAL SAMPLES\n",
      "####################################################################################################\n",
      "WARNING \t File for ttH125_gamgam not found in /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC\n",
      "WARNING \t File for ggH125_gamgam not found in /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC\n",
      "WARNING \t File for VBFH125_gamgam not found in /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC\n",
      "WARNING \t File for WpH125J_Wincl_gamgam not found in /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC\n",
      "WARNING \t File for ZH125J_Zincl_gamgam not found in /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC\n",
      "WARNING \t File data.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "###############################\n",
      "#### Background categories ####\n",
      "###############################\n",
      "Category             N(samples)\n",
      "-------------------------------\n",
      "Diboson                      10\n",
      "Higgs                        20\n",
      "Wjets                        42\n",
      "Wjetsincl                     6\n",
      "Zjets                        42\n",
      "Zjetsincl                     3\n",
      "singleTop                     6\n",
      "topX                          3\n",
      "ttbar                         1\n"
     ]
    }
   ],
   "source": [
    "mcfiles = initialize(opendatadir+\"/\"+analysis+\"/MC\")\n",
    "datafiles = initialize(opendatadir+\"/\"+analysis+\"/Data\")\n",
    "allfiles = z = {**mcfiles, **datafiles}\n",
    "Backgrounds = getBkgCategories()\n",
    "Backgrounds = [\"Diboson\", \"ttbar\", \"singleTop\", \"SUSYSlepSlep\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCcat = {}\n",
    "for cat in allfiles:\n",
    "    for dsid in allfiles[cat][\"dsid\"]:\n",
    "        try:\n",
    "            MCcat[int(dsid)] = cat\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not get DSID for /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC/Diboson.root. Skipping\n",
      "Could not get DSID for /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC/ttbar.root. Skipping\n",
      "Could not get DSID for /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC/singleTop.root. Skipping\n",
      "Could not get DSID for /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC/SUSYSlepSlep.root. Skipping\n",
      "Added 21867152 entries for backgrounds\n"
     ]
    }
   ],
   "source": [
    "dataset_IDs = []\n",
    "background.Reset()\n",
    "for b in Backgrounds:\n",
    "    i = 0\n",
    "    if not b in mcfiles.keys(): continue\n",
    "    for mc in mcfiles[b][\"files\"]:\n",
    "        if not os.path.isfile(mc): continue\n",
    "        try:\n",
    "            dataset_IDs.append(int(mcfiles[b][\"dsid\"][i]))\n",
    "            background.Add(mc)\n",
    "        except:\n",
    "            print(\"Could not get DSID for %s. Skipping\"%mc)\n",
    "        i += 1\n",
    "nen = background.GetEntries()\n",
    "print(\"Added %i entries for backgrounds\"%(nen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 24411580 entries for backgrounds\n"
     ]
    }
   ],
   "source": [
    "data.Reset(); \n",
    "for d in datafiles[\"data\"][\"files\"]:  \n",
    "    if not os.path.isfile(d): continue\n",
    "    data.Add(d)\n",
    "nen = data.GetEntries()\n",
    "print(\"Added %i entries for backgrounds\"%(nen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Making (a lot of) histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have read our dataset we want to start analyzing the data. To do so we need to put the data into histograms. For reasons that will become clear later in the analysis we must (for each variable) make one histogram per dataset ID. (We have 31 background samples, so if we want to study 10 variables we have to make 310 histograms!) For best dealing with all these histograms we can use dictionaries (Python) or maps (C++). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_mll = {}; hist_lep_pt = {}; hist_met = {}; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataset_IDs: \n",
    "    hist_mll[i] = R.TH1F() \n",
    "    hist_lep_pt[i] = R.TH1F()\n",
    "    hist_met[i] = R.TH1F()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataset_IDs: \n",
    "    hist_mll[i].SetNameTitle(\"hist_mll\", \"Invariant mass\"); \n",
    "    hist_lep_pt[i].SetNameTitle(\"hist_lep_pt\", \"Lepton pT\"); \n",
    "    hist_met[i].SetNameTitle(\"hist_met\", \"Missing ET\");\n",
    "    hist_mll[i].SetBins(20,0,500); \n",
    "    hist_lep_pt[i].SetBins(20,0,1000);\n",
    "    hist_met[i].SetBins(20,0,500); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data it is only necessary with one histogram for each variable: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_mll_d = R.TH1F(); \n",
    "hist_lep_pt_d = R.TH1F(); \n",
    "hist_met_d = R.TH1F(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_mll_d.SetNameTitle(\"hist_mll\", \"Invariant mass\"); \n",
    "hist_lep_pt_d.SetNameTitle(\"hist_lep_pt\", \"Lepton pT\"); \n",
    "hist_met_d.SetNameTitle(\"hist_met\", \"Missing ET\");\n",
    "hist_mll_d.SetBins(20,0,500); \n",
    "hist_lep_pt_d.SetBins(20,0,1000);\n",
    "hist_met_d.SetBins(20,0,500); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no stored variable or alias lumi\n"
     ]
    }
   ],
   "source": [
    "# Retrieve lumi from library\n",
    "%store -r lumi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Fill the histograms \n",
    "We can now loop over all events in our dataset, implement desired cuts, and fill the histograms we created above. In this example we choose only events containing exactly to same flavour leptons with opposite charge (i.e. $e^+e^-$ or $\\mu^+\\mu^-$). \n",
    "Before starting the loop we extract the total number of entries (events) in the TChain. We also make [TLorentzVector](https://root.cern.ch/doc/master/classTLorentzVector.html)s, which are very practical for handling the kinematics of the leptons, e.g. calculating the invariant mass of the two leptons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataset_IDs: \n",
    "    hist_mll[i].Reset(); \n",
    "    hist_lep_pt[i].Reset(); \n",
    "    hist_met[i].Reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_mll_d.Reset(); \n",
    "hist_lep_pt_d.Reset(); \n",
    "hist_met_d.Reset(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = R.TLorentzVector() \n",
    "l2 = R.TLorentzVector() \n",
    "dileptons = R.TLorentzVector() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the cell where the analysis is performed. Note that the cell needs to be run twice:\n",
    "\n",
    "1. with data = 0 to run over MC\n",
    "2. with data = 1 to run over data\n",
    "\n",
    "Note that the MC running takes ~5 minutes for 3lep analysis. Much(!!!) more time for e.g. 2lep analysis! Data running is relatively fast for 3lep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total events 100000/21867152\n",
      "Total events 200000/21867152\n",
      "Total events 300000/21867152\n",
      "Total events 400000/21867152\n",
      "Total events 500000/21867152\n",
      "Total events 600000/21867152\n",
      "Total events 700000/21867152\n",
      "Total events 800000/21867152\n",
      "Total events 900000/21867152\n",
      "Total events 1000000/21867152\n",
      "Total events 1100000/21867152\n",
      "Total events 1200000/21867152\n",
      "Total events 1300000/21867152\n",
      "Total events 1400000/21867152\n",
      "Total events 1500000/21867152\n",
      "Total events 1600000/21867152\n",
      "Total events 1700000/21867152\n",
      "Total events 1800000/21867152\n",
      "Total events 1900000/21867152\n",
      "Total events 2000000/21867152\n",
      "Total events 2100000/21867152\n",
      "Total events 2200000/21867152\n",
      "Total events 2300000/21867152\n",
      "Total events 2400000/21867152\n",
      "Total events 2500000/21867152\n",
      "Total events 2600000/21867152\n",
      "Total events 2700000/21867152\n",
      "Total events 2800000/21867152\n",
      "Total events 2900000/21867152\n",
      "Total events 3000000/21867152\n",
      "Total events 3100000/21867152\n",
      "Total events 3200000/21867152\n",
      "Total events 3300000/21867152\n",
      "Total events 3400000/21867152\n",
      "Total events 3500000/21867152\n",
      "Total events 3600000/21867152\n",
      "Total events 3700000/21867152\n",
      "Total events 3800000/21867152\n",
      "Total events 3900000/21867152\n",
      "Total events 4000000/21867152\n",
      "Total events 4100000/21867152\n",
      "Total events 4200000/21867152\n",
      "Total events 4300000/21867152\n",
      "Total events 4400000/21867152\n",
      "Total events 4500000/21867152\n",
      "Total events 4600000/21867152\n",
      "Total events 4700000/21867152\n",
      "Total events 4800000/21867152\n",
      "Total events 4900000/21867152\n",
      "Total events 5000000/21867152\n",
      "Total events 5100000/21867152\n",
      "Total events 5200000/21867152\n",
      "Total events 5300000/21867152\n",
      "Total events 5400000/21867152\n",
      "Total events 5500000/21867152\n",
      "Total events 5600000/21867152\n",
      "Total events 5700000/21867152\n",
      "Total events 5800000/21867152\n",
      "Total events 5900000/21867152\n",
      "Total events 6000000/21867152\n",
      "Total events 6100000/21867152\n",
      "Total events 6200000/21867152\n",
      "Total events 6300000/21867152\n",
      "Total events 6400000/21867152\n",
      "Total events 6500000/21867152\n",
      "Total events 6600000/21867152\n",
      "Total events 6700000/21867152\n",
      "Total events 6800000/21867152\n",
      "Total events 6900000/21867152\n",
      "Total events 7000000/21867152\n",
      "Total events 7100000/21867152\n",
      "Total events 7200000/21867152\n",
      "Total events 7300000/21867152\n",
      "Total events 7400000/21867152\n",
      "Total events 7500000/21867152\n",
      "Total events 7600000/21867152\n",
      "Total events 7700000/21867152\n",
      "Total events 7800000/21867152\n",
      "Total events 7900000/21867152\n",
      "Total events 8000000/21867152\n",
      "Total events 8100000/21867152\n",
      "Total events 8200000/21867152\n",
      "Total events 8300000/21867152\n",
      "Total events 8400000/21867152\n",
      "Total events 8500000/21867152\n",
      "Total events 8600000/21867152\n",
      "Total events 8700000/21867152\n",
      "Total events 8800000/21867152\n",
      "Total events 8900000/21867152\n",
      "Total events 9000000/21867152\n",
      "Total events 9100000/21867152\n",
      "Total events 9200000/21867152\n",
      "Total events 9300000/21867152\n",
      "Total events 9400000/21867152\n",
      "Total events 9500000/21867152\n",
      "Total events 9600000/21867152\n",
      "Total events 9700000/21867152\n",
      "Total events 9800000/21867152\n",
      "Total events 9900000/21867152\n",
      "Total events 10000000/21867152\n",
      "Total events 10100000/21867152\n",
      "Total events 10200000/21867152\n",
      "Total events 10300000/21867152\n",
      "Total events 10400000/21867152\n",
      "Total events 10500000/21867152\n",
      "Total events 10600000/21867152\n",
      "Total events 10700000/21867152\n",
      "Total events 10800000/21867152\n",
      "Total events 10900000/21867152\n",
      "Total events 11000000/21867152\n",
      "Total events 11100000/21867152\n",
      "Total events 11200000/21867152\n",
      "Total events 11300000/21867152\n",
      "Total events 11400000/21867152\n",
      "Total events 11500000/21867152\n",
      "Total events 11600000/21867152\n",
      "Total events 11700000/21867152\n",
      "Total events 11800000/21867152\n",
      "Total events 11900000/21867152\n",
      "Total events 12000000/21867152\n",
      "Total events 12100000/21867152\n",
      "Total events 12200000/21867152\n",
      "Total events 12300000/21867152\n",
      "Total events 12400000/21867152\n",
      "Total events 12500000/21867152\n",
      "Total events 12600000/21867152\n",
      "Total events 12700000/21867152\n",
      "Total events 12800000/21867152\n",
      "Total events 12900000/21867152\n",
      "Total events 13000000/21867152\n",
      "Total events 13100000/21867152\n",
      "Total events 13200000/21867152\n",
      "Total events 13300000/21867152\n",
      "Total events 13400000/21867152\n",
      "Total events 13500000/21867152\n",
      "Total events 13600000/21867152\n",
      "Total events 13700000/21867152\n",
      "Total events 13800000/21867152\n",
      "Total events 13900000/21867152\n",
      "Total events 14000000/21867152\n",
      "Total events 14100000/21867152\n",
      "Total events 14200000/21867152\n",
      "Total events 14300000/21867152\n",
      "Total events 14400000/21867152\n",
      "Total events 14500000/21867152\n",
      "Total events 14600000/21867152\n",
      "Total events 14700000/21867152\n",
      "Total events 14800000/21867152\n",
      "Total events 14900000/21867152\n",
      "Total events 15000000/21867152\n",
      "Total events 15100000/21867152\n",
      "Total events 15200000/21867152\n",
      "Total events 15300000/21867152\n",
      "Total events 15400000/21867152\n",
      "Total events 15500000/21867152\n",
      "Total events 15600000/21867152\n",
      "Total events 15700000/21867152\n",
      "Total events 15800000/21867152\n",
      "Total events 15900000/21867152\n",
      "Total events 16000000/21867152\n",
      "Total events 16100000/21867152\n",
      "Total events 16200000/21867152\n",
      "Total events 16300000/21867152\n",
      "Total events 16400000/21867152\n",
      "Total events 16500000/21867152\n",
      "Total events 16600000/21867152\n",
      "Total events 16700000/21867152\n",
      "Total events 16800000/21867152\n",
      "Total events 16900000/21867152\n",
      "Total events 17000000/21867152\n",
      "Total events 17100000/21867152\n",
      "Total events 17200000/21867152\n",
      "Total events 17300000/21867152\n",
      "Total events 17400000/21867152\n",
      "Total events 17500000/21867152\n",
      "Total events 17600000/21867152\n",
      "Total events 17700000/21867152\n",
      "Total events 17800000/21867152\n",
      "Total events 17900000/21867152\n",
      "Total events 18000000/21867152\n",
      "Total events 18100000/21867152\n",
      "Total events 18200000/21867152\n",
      "Total events 18300000/21867152\n",
      "Total events 18400000/21867152\n",
      "Total events 18500000/21867152\n",
      "Total events 18600000/21867152\n",
      "Total events 18700000/21867152\n",
      "Total events 18800000/21867152\n",
      "Total events 18900000/21867152\n",
      "Total events 19000000/21867152\n",
      "Total events 19100000/21867152\n",
      "Total events 19200000/21867152\n",
      "Total events 19300000/21867152\n",
      "Total events 19400000/21867152\n",
      "Total events 19500000/21867152\n",
      "Total events 19600000/21867152\n",
      "Total events 19700000/21867152\n",
      "Total events 19800000/21867152\n",
      "Total events 19900000/21867152\n",
      "Total events 20000000/21867152\n",
      "Total events 20100000/21867152\n",
      "Total events 20200000/21867152\n",
      "Total events 20300000/21867152\n",
      "Total events 20400000/21867152\n",
      "Total events 20500000/21867152\n",
      "Total events 20600000/21867152\n",
      "Total events 20700000/21867152\n",
      "Total events 20800000/21867152\n",
      "Total events 20900000/21867152\n",
      "Total events 21000000/21867152\n",
      "Total events 21100000/21867152\n",
      "Total events 21200000/21867152\n",
      "Total events 21300000/21867152\n",
      "Total events 21400000/21867152\n",
      "Total events 21500000/21867152\n",
      "Total events 21600000/21867152\n",
      "Total events 21700000/21867152\n",
      "Total events 21800000/21867152\n",
      "Done!\n",
      "CPU times: user 11min 26s, sys: 3.4 s, total: 11min 29s\n",
      "Wall time: 11min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of            lep_pt1  lep_eta1  lep_phi1       lep_E1    lep_m1     lep_pt2  \\\n",
       "0       617.628063 -0.456145  0.651721   683.004312 -0.112884  149.135312   \n",
       "1        71.277578 -0.980773  1.997837   108.396687 -0.027978   63.859563   \n",
       "2       624.291125 -1.800602 -3.065241  1941.074625  0.331861   35.573598   \n",
       "3       384.536656 -1.684730 -2.843123  1072.182250 -0.255588   39.422805   \n",
       "4       416.827063 -1.094879 -0.861139   692.641500 -0.134606   42.473062   \n",
       "...            ...       ...       ...          ...       ...         ...   \n",
       "283337  365.936906 -0.683010 -1.241499   454.662281  0.051307   60.022266   \n",
       "283338  634.140750  0.904956 -0.712485   912.015688  0.226749  521.212156   \n",
       "283339  245.439516  0.785956  2.223517   325.230406  0.042896  165.660297   \n",
       "283340  226.355875 -0.878349  1.779889   319.432469  0.072644  137.475063   \n",
       "283341  751.459250  0.168067  1.624177   762.097250  0.040237  644.191000   \n",
       "\n",
       "        lep_eta2  lep_phi2      lep_E2    lep_m2         met    weight  label  \n",
       "0      -0.130168  0.613360  150.400578  0.100729  147.908719  0.007713      0  \n",
       "1       0.921935 -1.752330   92.976289 -0.016234  104.189063  0.004763      0  \n",
       "2      -1.141782 -2.998903   61.392930  0.106475  130.091312  0.007829      0  \n",
       "3      -2.058256  2.663870  156.902250  0.102786  116.688766 -0.005958      0  \n",
       "4      -0.772170 -0.088552   55.777160  0.106523  122.859688  0.005958      0  \n",
       "...          ...       ...         ...       ...         ...       ...    ...  \n",
       "283337  0.788484 -2.817262   79.667297  0.017797  321.148438  0.000251      0  \n",
       "283338  0.313567  2.143383  547.046750  0.092215  159.677000  0.000251      0  \n",
       "283339 -0.228129  1.246808  169.989781  0.097674  515.265094  0.000251      0  \n",
       "283340 -0.791179  0.926855  182.794109  0.091856  342.513531  0.000238      0  \n",
       "283341 -0.807636 -0.982910  865.957437  0.118396  323.088719  0.000238      0  \n",
       "\n",
       "[283342 rows x 13 columns]>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "from math import sin\n",
    "isData = 0; \n",
    "\n",
    "if isData == 1: ds = data \n",
    "else: ds = background\n",
    "    \n",
    "columns = {\"lep_pt1\":[],\"lep_eta1\":[],\"lep_phi1\":[],\"lep_E1\":[], \"lep_m1\":[],\n",
    "           \"lep_pt2\":[],\"lep_eta2\":[],\"lep_phi2\":[],\"lep_E2\":[], \"lep_m2\":[],\n",
    "           \"met\":[], \"weight\": [], \"label\": []}\n",
    "\n",
    "i = 0   \n",
    "for event in ds: \n",
    "    \n",
    "    if i%100000 == 0 and i>0: \n",
    "        print(\"Total events %i/%i\"%(i,ds.GetEntries()))\n",
    "    i += 1 \n",
    "    ## Data quality cuts: \n",
    "    \n",
    "    #if i > 10000000: break\n",
    "    \n",
    "    if(ds.trigM == 0 and ds.trigE == 0):continue  \n",
    "\n",
    "    ## Event selection:    \n",
    "    ## Cut #1: Require (exactly) 2 leptons.\n",
    "    if not ds.lep_n == 2: continue\n",
    "    ## Cut #2: Require opposite charge.\n",
    "    if ds.lep_charge[0] == ds.lep_charge[1]: continue\n",
    "    ## Cut #3: Require same flavour (2 electrons or 2 muons).\n",
    "    if not ds.lep_type[0] == ds.lep_type[1]: continue\n",
    "    ## Cut #4: Require a significant amount of missing energy.\n",
    "    \n",
    "    #New cuts\n",
    "    if ds.met_et/1000.0 < 100: continue\n",
    "    \n",
    "    ## Require \"good leptons\": \n",
    "    if ds.lep_pt[0]/1000.0 < 25: continue\n",
    "    if ds.lep_etcone20[0]/ds.lep_pt[0] > 0.15: continue\n",
    "    if ds.lep_ptcone30[0]/ds.lep_pt[0] > 0.15: continue\n",
    "        \n",
    "    if ds.lep_pt[1]/1000.0 < 25: continue\n",
    "    if ds.lep_etcone20[1]/ds.lep_pt[1] > 0.15: continue\n",
    "    if ds.lep_ptcone30[1]/ds.lep_pt[1] > 0.15: continue\n",
    "    \n",
    "    #New cuts\n",
    "    ## Require loose criteria.\n",
    "    if ds.lep_type[0] == 11:\n",
    "        if (abs(ds.lep_eta[0]) > 2.47 or abs(ds.lep_eta[1]) > 2.47 ) : continue\n",
    "        if abs(ds.lep_trackd0pvunbiased[0] / ds.lep_tracksigd0pvunbiased[0]) > 5: continue\n",
    "        if abs(ds.lep_trackd0pvunbiased[1] / ds.lep_tracksigd0pvunbiased[1]) > 5: continue\n",
    "    if ds.lep_type[0] == 13:\n",
    "        if (abs(ds.lep_eta[0]) > 2.7 or abs(ds.lep_eta[1]) > 2.7 ) : continue\n",
    "        if abs(ds.lep_trackd0pvunbiased[0] / ds.lep_tracksigd0pvunbiased[0]) > 3: continue\n",
    "        if abs(ds.lep_trackd0pvunbiased[1] / ds.lep_tracksigd0pvunbiased[1]) > 3: continue\n",
    "    \n",
    "    \n",
    "    ## Set Lorentz vectors: \n",
    "    l1.SetPtEtaPhiE(ds.lep_pt[0]/1000., ds.lep_eta[0], ds.lep_phi[0], ds.lep_E[0]/1000.);\n",
    "    l2.SetPtEtaPhiE(ds.lep_pt[1]/1000., ds.lep_eta[1], ds.lep_phi[1], ds.lep_E[1]/1000.);\n",
    "    \n",
    "    #New cuts\n",
    "    if abs(ds.lep_z0[0] * sin(l1.Theta())) > 0.5: continue \n",
    "    if abs(ds.lep_z0[1] * sin(l2.Theta())) > 0.5: continue\n",
    "    \n",
    "    ## Variables are stored in the TTree with unit MeV, so we need to divide by 1000 \n",
    "    ## to get GeV, which is a more practical and commonly used unit. \n",
    "    dileptons = l1 + l2;   \n",
    "    if dileptons.M()<100:continue\n",
    "\n",
    "    columns[\"lep_pt1\"].append(ds.lep_pt[0]/1000.0)\n",
    "    columns[\"lep_eta1\"].append(ds.lep_eta[0])\n",
    "    columns[\"lep_phi1\"].append(ds.lep_phi[0])\n",
    "    columns[\"lep_E1\"].append(ds.lep_E[0]/1000.0)\n",
    "    columns[\"lep_m1\"].append(l1.M())\n",
    "    \n",
    "    columns[\"lep_pt2\"].append(ds.lep_pt[1]/1000.0)\n",
    "    columns[\"lep_eta2\"].append(ds.lep_eta[1])\n",
    "    columns[\"lep_phi2\"].append(ds.lep_phi[1])\n",
    "    columns[\"lep_E2\"].append(ds.lep_E[1]/1000.0)\n",
    "    columns[\"lep_m2\"].append(l2.M())\n",
    "    \n",
    "    columns[\"met\"].append(ds.met_et/1000.0) \n",
    "    \n",
    "    if ds.channelNumber == 392918:\n",
    "        columns[\"label\"].append(1)\n",
    "    else:\n",
    "        columns[\"label\"].append(0)\n",
    "    if isData == 0:\n",
    "        weight = ((ds.mcWeight)*(ds.scaleFactor_PILEUP)*\n",
    "                 (1)*(1)*\n",
    "                 (1))*((ds.XSection*lumi)/ds.SumWeights)\n",
    "        columns[\"weight\"].append(weight)\n",
    "    else:\n",
    "        columns[\"weight\"].append(-999.0)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "print(\"Done!\")\n",
    "df = pd.DataFrame(data=columns)\n",
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scale and classify the histograms (MC only) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are ready to make plots we need to do some further processing of the histograms we made above. The information necessary for doing the two steps below is found in the file **Infofile.txt**.   \n",
    "1. We need to **scale** the histograms to the right cross section and luminosity. Why? When making the MC samples a certain number of events is simulated, which will usually not correspond to the number of events in our data. The expected number of events from a certain kind of process is given by $N=\\sigma L$, where $\\sigma$ is the cross section and $L$ is the integrated luminosity. Therefore we need to scale each histogram by a scale factor <br> <br>\n",
    "$$sf = \\frac{N}{N_{MC}} = \\frac{ \\sigma L }{N_{MC}},$$ <br>  where $N_{MC}$ is the number of generated MC events.  <br> <br>\n",
    "2. We also need to **classify** the background processes into different categories. This is necessary when we eventually want to make the characteristic colorful background plots you might have seen before.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Make new histograms \n",
    "Maybe a bit depressingly we have to make a set of new histograms, this time corresponding to the different background categories, instead of the dataset IDs. Notice that these new histograms are made in a very similar way as above, i.e. with the same range and binning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_mll = {}; H_lep_pt = {}; H_met = {}; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Backgrounds: \n",
    "    H_mll[i] = R.TH1F() \n",
    "    H_lep_pt[i] = R.TH1F() \n",
    "    H_met[i] = R.TH1F() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Backgrounds: \n",
    "    H_mll[i].SetNameTitle(\"hist_mll\", \"Invariant mass\"); \n",
    "    H_lep_pt[i].SetNameTitle(\"hist_lep_pt\", \"Lepton pT\"); \n",
    "    H_met[i].SetNameTitle(\"hist_met\", \"Missing ET\");\n",
    "    H_mll[i].SetBins(20,0,500); \n",
    "    H_lep_pt[i].SetBins(20,0,1000);\n",
    "    H_met[i].SetBins(20,0,500); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Scale and add histograms \n",
    "Now we read our info file, scale all (old) histograms, and then add them to the new histograms we just defined.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Backgrounds: \n",
    "    H_mll[i].Reset(); \n",
    "    H_lep_pt[i].Reset(); \n",
    "    H_met[i].Reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dsid in hist_mll.keys(): \n",
    "    \n",
    "    Type = MCcat[dsid]\n",
    "    \n",
    "    H_mll[Type].Add(hist_mll[dsid]); \n",
    "    H_lep_pt[Type].Add(hist_lep_pt[dsid]); \n",
    "    H_met[Type].Add(hist_met[dsid]); \n",
    "    \n",
    "infofile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Color the histograms \n",
    "Make yet another map, this time containing the colors you want the backgrounds to have, and then set the colors of your histograms. Note that colors are defined by integers in ROOT. If you are not happy with the colors chosen below you can have look at the [TColor](https://root.cern.ch/doc/master/classTColor.html) class reference for more options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors[\"Diboson\"] = R.kGreen; \n",
    "colors[\"Zjets\"] = R.kYellow; \n",
    "colors[\"ttbar\"] = R.kRed;\n",
    "colors[\"singleTop\"] = R.kBlue-7; \n",
    "colors[\"Wjets\"] = R.kBlue+3; \n",
    "colors[\"topX\"] = R.kOrange+1; \n",
    "colors[\"Higgs\"] = R.kMagenta; \n",
    "colors[\"Wjetsincl\"] = R.kBlue-10;\n",
    "colors[\"Zjetsincl\"] = R.kYellow-9;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in Backgrounds: \n",
    "    H_mll[h].SetFillColor(colors[h]); \n",
    "    H_met[h].SetFillColor(colors[h]);\n",
    "    H_lep_pt[h].SetFillColor(colors[h]);\n",
    "    \n",
    "    H_mll[h].SetLineColor(colors[h]); \n",
    "    H_met[h].SetLineColor(colors[h]);\n",
    "    H_lep_pt[h].SetLineColor(colors[h]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stack and plot the histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have arrived to the part where we can plot the results of all the work done above. For each variable we need to stack the backgrounds on top of each other, which is done by using the [THStack](https://root.cern.ch/doc/master/classTHStack.html) class. In the example below we do this for two variables; invariant mass and missing $E_T$.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_mll = R.THStack(\"Invariant mass\", \"\");\n",
    "stack_met = R.THStack(\"Missing ET\", \"\"); \n",
    "stack_lep_pt = R.THStack(\"Lepton pT\", \"\"); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in Backgrounds: \n",
    "    stack_mll.RecursiveRemove(H_mll[h]); ## Remove previously stacked histograms  \n",
    "    stack_met.RecursiveRemove(H_met[h]);\n",
    "    stack_lep_pt.RecursiveRemove(H_lep_pt[h]);\n",
    "    stack_mll.Add(H_mll[h]); \n",
    "    stack_met.Add(H_met[h]);\n",
    "    stack_lep_pt.Add(H_lep_pt[h]); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make a legend (see [TLegend](https://root.cern.ch/doc/master/classTLegend.html)), and add  the different backgrounds. Next we make a canvas (see [TCanvas](https://root.cern.ch/doc/master/classTCanvas.html)), which is allways necessary when we want to make a plot. Then you draw the stack and the legend, and display them by drawing the canvas. We can also specify axis labels and a bunch of other stuff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R.gStyle.SetLegendBorderSize(0); ## Remove (default) border around legend \n",
    "leg = R.TLegend(0.65, 0.60, 0.9, 0.85); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leg.Clear();\n",
    "for i in Backgrounds: \n",
    "    leg.AddEntry(H_mll[i], i, \"f\")  ## Add your histograms to the legend\n",
    "leg.AddEntry(hist_mll_d, \"Data\", \"lep\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = R.TCanvas(\"c\", \"c\", 600, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R.gPad.SetLogy() ## Set logarithmic y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_mll_d.SetLineColor(R.kBlack); \n",
    "hist_mll_d.SetMarkerStyle(R.kFullCircle); \n",
    "hist_mll_d.SetMarkerColor(R.kBlack); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_mll.Draw(\"hist\"); \n",
    "stack_mll.SetMaximum(1E6); \n",
    "stack_mll.SetMinimum(1); \n",
    "stack_mll.GetYaxis().SetTitle(\"# events\");\n",
    "stack_mll.GetYaxis().SetTitleOffset(1.3); \n",
    "stack_mll.GetXaxis().SetTitle(\"m_{ll} (GeV)\");\n",
    "stack_mll.GetXaxis().SetTitleOffset(1.3);\n",
    "hist_mll_d.Draw(\"same E\"); \n",
    "leg.Draw();\n",
    "C.Draw();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_met_d.SetLineColor(R.kBlack); \n",
    "hist_met_d.SetMarkerStyle(R.kFullCircle); \n",
    "hist_met_d.SetMarkerColor(R.kBlack); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_met.Draw(\"hist\"); \n",
    "stack_met.SetMaximum(1E6); \n",
    "stack_met.GetYaxis().SetTitle(\"# events\");\n",
    "stack_met.GetYaxis().SetTitleOffset(1.3); \n",
    "stack_met.GetXaxis().SetTitle(\"E_{T}^{miss} (GeV)\");\n",
    "stack_met.GetXaxis().SetTitleOffset(1.3);\n",
    "hist_met_d.Draw(\"same e\"); \n",
    "leg.Draw();\n",
    "C.Draw(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_lep_pt_d.SetLineColor(R.kBlack); \n",
    "hist_lep_pt_d.SetMarkerStyle(R.kFullCircle); \n",
    "hist_lep_pt_d.SetMarkerColor(R.kBlack); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_lep_pt.Draw(\"hist\"); \n",
    "stack_lep_pt.SetMaximum(1E6); \n",
    "stack_lep_pt.GetYaxis().SetTitle(\"# events\");\n",
    "stack_lep_pt.GetYaxis().SetTitleOffset(1.3); \n",
    "stack_lep_pt.GetXaxis().SetTitle(\"p_{T} (GeV)\");\n",
    "stack_lep_pt.GetXaxis().SetTitleOffset(1.3);\n",
    "hist_lep_pt_d.Draw(\"same e\"); \n",
    "leg.Draw();\n",
    "C.Draw(); "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
