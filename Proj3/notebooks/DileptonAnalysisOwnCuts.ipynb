{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When running this notebook via the Galaxy portal\n",
    "You can access your data via the dataset number. Using a Python kernel, you can access dataset number 42 with ``handle = open(get(42), 'r')``.\n",
    "To save data, write your data to a file, and then call ``put('filename.txt')``. The dataset will then be available in your galaxy history.\n",
    "<br><br>Note that if you are putting/getting to/from a different history than your default history, you must also provide the history-id.\n",
    "<br><br>More information including available galaxy-related environment variables can be found at https://github.com/bgruening/docker-jupyter-notebook. This notebook is running in a docker container based on the Docker Jupyter container described in that link.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dilepton analysis  (Python) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of a simple dilepton analysis, quite similar to the notebook called \"Dilepton_analysis_noData.ipynb\", but with some differences. The most obvious difference is that we here also include real data. This example also has a slightly more advanced event selection.  \n",
    "\n",
    "**Notice:** This is *only an example* on how to do this. Feel free to be creative, and to find better and/or more elegant ways of doing the various steps! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.24/02\n",
      "importing Jupyter notebook from setPath.ipynb\n",
      "importing Jupyter notebook from /storage/galaxy/jobs_directory/003/3105/working/jupyter/Input/OpenDataPandaFramework13TeV.ipynb\n",
      "This library contains handy functions to ease the access and use of the 13TeV ATLAS OpenData release\n",
      "\n",
      "getBkgCategories()\n",
      "\t Dumps the name of the various background cataegories available \n",
      "\t as well as the number of samples contained in each category.\n",
      "\t Returns a vector with the name of the categories\n",
      "\n",
      "getSamplesInCategory(cat)\n",
      "\t Dumps the name of the samples contained in a given category (cat)\n",
      "\t Returns dictionary with keys being DSIDs and values physics process name from filename.\n",
      "\n",
      "getMCCategory()\n",
      "\t Returns dictionary with keys DSID and values MC category\n",
      "\n",
      "initialize(indir)\n",
      "\t Collects all the root files available in a certain directory (indir)\n",
      "\n",
      "\n",
      "\n",
      "Setting luminosity to 10064 pb^-1\n",
      "\n",
      "###############################\n",
      "#### Background categories ####\n",
      "###############################\n",
      "Category             N(samples)\n",
      "-------------------------------\n",
      "Diboson                      10\n",
      "Higgs                        20\n",
      "Wjets                        42\n",
      "Wjetsincl                     6\n",
      "Zjets                        42\n",
      "Zjetsincl                     3\n",
      "singleTop                     6\n",
      "topX                          3\n",
      "ttbar                         1\n",
      "###############################\n",
      "#### Signal categories ####\n",
      "###############################\n",
      "Category             N(samples)\n",
      "-------------------------------\n",
      "GG_ttn1                       4\n",
      "Gee                           5\n",
      "Gmumu                         5\n",
      "RS_G_ZZ                       5\n",
      "SUSYC1C1                     10\n",
      "SUSYC1N2                     18\n",
      "SUSYSlepSlep                 14\n",
      "TT_directTT                   4\n",
      "ZPrimeee                      4\n",
      "ZPrimemumu                    4\n",
      "ZPrimett                     12\n",
      "dmV_Zll                      10\n"
     ]
    }
   ],
   "source": [
    "import ROOT as R\n",
    "import import_ipynb\n",
    "import setPath\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from Input.OpenDataPandaFramework13TeV import *\n",
    "%jsroot on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the analaysis to run (*1largeRjet1lep*, *1lep1tau*, *3lep*, *exactly2lep*, *GamGam*, *2lep*, *4lep*)\n",
    "\n",
    "Set the directory where you have downloaded the ATLAS OpenData samples you want to run over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "opendatadir = \"/storage/shared/data/fys5555/ATLAS_opendata/\"\n",
    "analysis = \"2lep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = R.TChain(\"mini\")\n",
    "data = R.TChain(\"mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of all the background samples, category and their IDs can be found in **Infofile.txt**. The cross-section, efficiencies etc. needed for scaling are stored in the **Files_<---->**. We read these files and add all the samples to the TChain. We also (for later convenience) make a vector containing the dataset IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################################\n",
      "BACKGROUND SAMPLES\n",
      "####################################################################################################\n",
      "WARNING \t File for ggH125_tautaulh not found in /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC\n",
      "WARNING \t File for VBFH125_tautaulh not found in /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC\n",
      "####################################################################################################\n",
      "SIGNAL SAMPLES\n",
      "####################################################################################################\n",
      "WARNING \t File for ttH125_gamgam not found in /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC\n",
      "WARNING \t File for ggH125_gamgam not found in /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC\n",
      "WARNING \t File for VBFH125_gamgam not found in /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC\n",
      "WARNING \t File for WpH125J_Wincl_gamgam not found in /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC\n",
      "WARNING \t File for ZH125J_Zincl_gamgam not found in /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC\n",
      "WARNING \t File data.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "###############################\n",
      "#### Background categories ####\n",
      "###############################\n",
      "Category             N(samples)\n",
      "-------------------------------\n",
      "Diboson                      10\n",
      "Higgs                        20\n",
      "Wjets                        42\n",
      "Wjetsincl                     6\n",
      "Zjets                        42\n",
      "Zjetsincl                     3\n",
      "singleTop                     6\n",
      "topX                          3\n",
      "ttbar                         1\n"
     ]
    }
   ],
   "source": [
    "mcfiles = initialize(opendatadir+\"/\"+analysis+\"/MC\")\n",
    "datafiles = initialize(opendatadir+\"/\"+analysis+\"/Data\")\n",
    "allfiles = z = {**mcfiles, **datafiles}\n",
    "Backgrounds = getBkgCategories()\n",
    "Backgrounds.append(\"SUSYSlepSlep\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCcat = {}\n",
    "for cat in allfiles:\n",
    "    for dsid in allfiles[cat][\"dsid\"]:\n",
    "        try:\n",
    "            MCcat[int(dsid)] = cat\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 118376331 entries for backgrounds\n"
     ]
    }
   ],
   "source": [
    "dataset_IDs = []\n",
    "background.Reset()\n",
    "for b in Backgrounds:\n",
    "    i = 0\n",
    "    if not b in mcfiles.keys(): continue\n",
    "    for mc in mcfiles[b][\"files\"]:\n",
    "        if not os.path.isfile(mc): continue\n",
    "        try:\n",
    "            dataset_IDs.append(int(mcfiles[b][\"dsid\"][i]))\n",
    "            background.Add(mc)\n",
    "        except:\n",
    "            print(\"Could not get DSID for %s. Skipping\"%mc)\n",
    "        i += 1\n",
    "nen = background.GetEntries()\n",
    "print(\"Added %i entries for backgrounds\"%(nen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 24411580 entries for backgrounds\n"
     ]
    }
   ],
   "source": [
    "data.Reset(); \n",
    "for d in datafiles[\"data\"][\"files\"]:  \n",
    "    if not os.path.isfile(d): continue\n",
    "    data.Add(d)\n",
    "nen = data.GetEntries()\n",
    "print(\"Added %i entries for backgrounds\"%(nen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Making (a lot of) histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have read our dataset we want to start analyzing the data. To do so we need to put the data into histograms. For reasons that will become clear later in the analysis we must (for each variable) make one histogram per dataset ID. (We have 31 background samples, so if we want to study 10 variables we have to make 310 histograms!) For best dealing with all these histograms we can use dictionaries (Python) or maps (C++). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_VZ = {}; hist_WW = {}; hist_Top = {}; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataset_IDs: \n",
    "    hist_VZ[i] = R.TH1F() \n",
    "    hist_WW[i] = R.TH1F()\n",
    "    hist_Top[i] = R.TH1F()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataset_IDs: \n",
    "    hist_VZ[i].SetNameTitle(\"hist_VZ\", \"Invariant mass, VZ\"); \n",
    "    hist_WW[i].SetNameTitle(\"hist_WW\", \"Invariant mass, WW\"); \n",
    "    hist_Top[i].SetNameTitle(\"hist_Top\", \"Invariant mass, Top\");\n",
    "    hist_VZ[i].SetBins(20,0,300); \n",
    "    hist_WW[i].SetBins(20,0,300);\n",
    "    hist_Top[i].SetBins(20,0,300); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data it is only necessary with one histogram for each variable: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_VZ_d = R.TH1F(); \n",
    "hist_WW_d = R.TH1F(); \n",
    "hist_Top_d = R.TH1F(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_VZ_d.SetNameTitle(\"hist_VZ\", \"Invariant mass, VZ\"); \n",
    "hist_WW_d.SetNameTitle(\"hist_WW\", \"Invariant mass, WW\"); \n",
    "hist_Top_d.SetNameTitle(\"hist_Top\", \"Invariant mass, Top\");\n",
    "hist_VZ_d.SetBins(20,0,300); \n",
    "hist_WW_d.SetBins(20,0,300);\n",
    "hist_Top_d.SetBins(20,0,300); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no stored variable or alias lumi\n"
     ]
    }
   ],
   "source": [
    "# Retrieve lumi from library\n",
    "%store -r lumi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Fill the histograms \n",
    "We can now loop over all events in our dataset, implement desired cuts, and fill the histograms we created above. In this example we choose only events containing exactly to same flavour leptons with opposite charge (i.e. $e^+e^-$ or $\\mu^+\\mu^-$). \n",
    "Before starting the loop we extract the total number of entries (events) in the TChain. We also make [TLorentzVector](https://root.cern.ch/doc/master/classTLorentzVector.html)s, which are very practical for handling the kinematics of the leptons, e.g. calculating the invariant mass of the two leptons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataset_IDs: \n",
    "    hist_VZ[i].Reset(); \n",
    "    hist_WW[i].Reset(); \n",
    "    hist_Top[i].Reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_VZ_d.Reset(); \n",
    "hist_WW_d.Reset(); \n",
    "hist_Top_d.Reset(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = R.TLorentzVector() \n",
    "l2 = R.TLorentzVector() \n",
    "dileptons = R.TLorentzVector() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the cell where the analysis is performed. Note that the cell needs to be run twice:\n",
    "\n",
    "1. with data = 0 to run over MC\n",
    "2. with data = 1 to run over data\n",
    "\n",
    "Note that the MC running takes ~5 minutes for 3lep analysis. Much(!!!) more time for e.g. 2lep analysis! Data running is relatively fast for 3lep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41% completed and 99.023% cut.\n",
      "WW: 65, VZ: 58, top: 22\n",
      "0.82% completed and 99.095% cut.\n",
      "WW: 120, VZ: 105, top: 42\n",
      "1.23% completed and 99.161% cut.\n",
      "WW: 174, VZ: 144, top: 62\n",
      "1.64% completed and 99.191% cut.\n",
      "WW: 217, VZ: 179, top: 81\n",
      "2.05% completed and 99.311% cut.\n",
      "WW: 240, VZ: 200, top: 101\n",
      "2.46% completed and 99.315% cut.\n",
      "WW: 270, VZ: 221, top: 127\n",
      "2.87% completed and 99.047% cut.\n",
      "WW: 327, VZ: 268, top: 145\n",
      "3.28% completed and 98.864% cut.\n",
      "WW: 409, VZ: 357, top: 169\n",
      "3.69% completed and 99.075% cut.\n",
      "WW: 472, VZ: 404, top: 189\n",
      "4.10% completed and 99.178% cut.\n",
      "WW: 518, VZ: 439, top: 211\n",
      "4.51% completed and 99.122% cut.\n",
      "WW: 574, VZ: 483, top: 235\n",
      "4.92% completed and 98.980% cut.\n",
      "WW: 656, VZ: 534, top: 247\n",
      "5.33% completed and 99.022% cut.\n",
      "WW: 738, VZ: 584, top: 265\n",
      "5.73% completed and 99.143% cut.\n",
      "WW: 793, VZ: 632, top: 281\n",
      "6.14% completed and 98.962% cut.\n",
      "WW: 861, VZ: 694, top: 313\n",
      "6.55% completed and 99.143% cut.\n",
      "WW: 915, VZ: 730, top: 336\n",
      "6.96% completed and 99.186% cut.\n",
      "WW: 958, VZ: 757, top: 362\n",
      "7.37% completed and 99.036% cut.\n",
      "WW: 1044, VZ: 836, top: 383\n",
      "7.78% completed and 98.973% cut.\n",
      "WW: 1112, VZ: 898, top: 412\n",
      "8.19% completed and 98.937% cut.\n",
      "WW: 1183, VZ: 959, top: 432\n",
      "8.60% completed and 98.956% cut.\n",
      "WW: 1247, VZ: 1026, top: 459\n",
      "9.01% completed and 98.967% cut.\n",
      "WW: 1331, VZ: 1090, top: 476\n",
      "9.42% completed and 99.117% cut.\n",
      "WW: 1376, VZ: 1127, top: 500\n",
      "9.83% completed and 99.161% cut.\n",
      "WW: 1418, VZ: 1174, top: 521\n",
      "10.24% completed and 99.207% cut.\n",
      "WW: 1471, VZ: 1202, top: 541\n",
      "10.65% completed and 98.934% cut.\n",
      "WW: 1565, VZ: 1258, top: 560\n",
      "11.06% completed and 99.027% cut.\n",
      "WW: 1628, VZ: 1307, top: 583\n",
      "11.47% completed and 98.882% cut.\n",
      "WW: 1700, VZ: 1374, top: 612\n",
      "11.88% completed and 99.028% cut.\n",
      "WW: 1768, VZ: 1427, top: 631\n",
      "12.29% completed and 98.863% cut.\n",
      "WW: 1857, VZ: 1489, top: 654\n",
      "12.70% completed and 98.929% cut.\n",
      "WW: 1942, VZ: 1546, top: 678\n",
      "13.11% completed and 98.818% cut.\n",
      "WW: 2035, VZ: 1622, top: 695\n",
      "13.52% completed and 98.856% cut.\n",
      "WW: 2117, VZ: 1709, top: 721\n",
      "13.93% completed and 99.225% cut.\n",
      "WW: 2160, VZ: 1746, top: 736\n",
      "14.34% completed and 99.075% cut.\n",
      "WW: 2216, VZ: 1786, top: 759\n",
      "14.75% completed and 99.207% cut.\n",
      "WW: 2252, VZ: 1819, top: 783\n",
      "15.16% completed and 99.040% cut.\n",
      "WW: 2308, VZ: 1868, top: 802\n",
      "15.57% completed and 99.024% cut.\n",
      "WW: 2373, VZ: 1922, top: 828\n",
      "15.98% completed and 98.885% cut.\n",
      "WW: 2455, VZ: 1988, top: 853\n",
      "16.39% completed and 99.127% cut.\n",
      "WW: 2500, VZ: 2034, top: 876\n",
      "16.80% completed and 99.170% cut.\n",
      "WW: 2537, VZ: 2056, top: 900\n",
      "17.20% completed and 99.139% cut.\n",
      "WW: 2591, VZ: 2103, top: 932\n",
      "17.61% completed and 99.198% cut.\n",
      "WW: 2637, VZ: 2140, top: 953\n",
      "18.02% completed and 98.803% cut.\n",
      "WW: 2739, VZ: 2226, top: 971\n",
      "18.43% completed and 98.923% cut.\n",
      "WW: 2821, VZ: 2285, top: 991\n",
      "18.84% completed and 98.850% cut.\n",
      "WW: 2921, VZ: 2358, top: 1023\n",
      "19.25% completed and 98.799% cut.\n",
      "WW: 2992, VZ: 2433, top: 1046\n",
      "19.66% completed and 98.679% cut.\n",
      "WW: 3115, VZ: 2529, top: 1067\n",
      "20.07% completed and 98.680% cut.\n",
      "WW: 3226, VZ: 2614, top: 1090\n",
      "20.48% completed and 98.798% cut.\n",
      "WW: 3321, VZ: 2689, top: 1116\n",
      "20.89% completed and 99.016% cut.\n",
      "WW: 3390, VZ: 2752, top: 1134\n",
      "21.30% completed and 99.062% cut.\n",
      "WW: 3457, VZ: 2788, top: 1159\n",
      "21.71% completed and 98.981% cut.\n",
      "WW: 3538, VZ: 2842, top: 1180\n",
      "22.12% completed and 98.760% cut.\n",
      "WW: 3662, VZ: 2914, top: 1201\n",
      "22.53% completed and 98.891% cut.\n",
      "WW: 3748, VZ: 2995, top: 1221\n",
      "22.94% completed and 98.961% cut.\n",
      "WW: 3818, VZ: 3041, top: 1244\n",
      "23.35% completed and 99.035% cut.\n",
      "WW: 3892, VZ: 3098, top: 1269\n",
      "23.76% completed and 98.825% cut.\n",
      "WW: 3983, VZ: 3166, top: 1303\n",
      "24.17% completed and 98.637% cut.\n",
      "WW: 4104, VZ: 3262, top: 1329\n",
      "24.58% completed and 99.006% cut.\n",
      "WW: 4173, VZ: 3316, top: 1359\n",
      "24.99% completed and 98.854% cut.\n",
      "WW: 4248, VZ: 3384, top: 1377\n",
      "25.40% completed and 99.077% cut.\n",
      "WW: 4309, VZ: 3431, top: 1394\n",
      "25.81% completed and 99.136% cut.\n",
      "WW: 4357, VZ: 3468, top: 1417\n",
      "26.22% completed and 98.944% cut.\n",
      "WW: 4439, VZ: 3534, top: 1446\n",
      "26.63% completed and 99.080% cut.\n",
      "WW: 4499, VZ: 3588, top: 1469\n",
      "27.04% completed and 99.234% cut.\n",
      "WW: 4530, VZ: 3620, top: 1495\n",
      "27.45% completed and 99.140% cut.\n",
      "WW: 4587, VZ: 3666, top: 1524\n",
      "27.86% completed and 98.464% cut.\n",
      "WW: 4752, VZ: 3775, top: 1552\n",
      "28.27% completed and 98.387% cut.\n",
      "WW: 4884, VZ: 3890, top: 1576\n",
      "28.67% completed and 98.596% cut.\n",
      "WW: 5014, VZ: 3989, top: 1600\n",
      "29.08% completed and 98.765% cut.\n",
      "WW: 5106, VZ: 4063, top: 1626\n",
      "29.49% completed and 98.625% cut.\n",
      "WW: 5211, VZ: 4143, top: 1649\n",
      "29.90% completed and 98.594% cut.\n",
      "WW: 5324, VZ: 4248, top: 1663\n",
      "30.31% completed and 98.746% cut.\n",
      "WW: 5416, VZ: 4335, top: 1686\n",
      "30.72% completed and 99.109% cut.\n",
      "WW: 5475, VZ: 4387, top: 1708\n",
      "31.13% completed and 99.033% cut.\n",
      "WW: 5539, VZ: 4447, top: 1730\n",
      "31.54% completed and 99.124% cut.\n",
      "WW: 5608, VZ: 4492, top: 1749\n",
      "31.95% completed and 98.844% cut.\n",
      "WW: 5692, VZ: 4573, top: 1771\n",
      "32.36% completed and 99.136% cut.\n",
      "WW: 5736, VZ: 4619, top: 1784\n",
      "32.77% completed and 99.107% cut.\n",
      "WW: 5788, VZ: 4664, top: 1806\n",
      "33.18% completed and 98.290% cut.\n",
      "WW: 5970, VZ: 4814, top: 1834\n",
      "33.59% completed and 98.504% cut.\n",
      "WW: 6075, VZ: 4931, top: 1861\n",
      "34.00% completed and 99.059% cut.\n",
      "WW: 6131, VZ: 4984, top: 1880\n",
      "34.41% completed and 98.936% cut.\n",
      "WW: 6202, VZ: 5046, top: 1897\n",
      "34.82% completed and 98.968% cut.\n",
      "WW: 6271, VZ: 5103, top: 1923\n",
      "35.23% completed and 98.849% cut.\n",
      "WW: 6352, VZ: 5181, top: 1950\n",
      "35.64% completed and 98.521% cut.\n",
      "WW: 6472, VZ: 5276, top: 1974\n",
      "36.05% completed and 98.733% cut.\n",
      "WW: 6566, VZ: 5355, top: 1998\n",
      "36.46% completed and 98.998% cut.\n",
      "WW: 6631, VZ: 5401, top: 2015\n",
      "36.87% completed and 98.605% cut.\n",
      "WW: 6769, VZ: 5517, top: 2039\n",
      "37.28% completed and 98.594% cut.\n",
      "WW: 6909, VZ: 5631, top: 2066\n",
      "37.69% completed and 98.823% cut.\n",
      "WW: 7005, VZ: 5711, top: 2088\n",
      "38.10% completed and 98.611% cut.\n",
      "WW: 7120, VZ: 5809, top: 2104\n",
      "38.51% completed and 98.968% cut.\n",
      "WW: 7190, VZ: 5863, top: 2132\n",
      "38.92% completed and 98.755% cut.\n",
      "WW: 7267, VZ: 5926, top: 2158\n",
      "39.33% completed and 98.998% cut.\n",
      "WW: 7317, VZ: 5965, top: 2187\n",
      "39.74% completed and 99.127% cut.\n",
      "WW: 7377, VZ: 6007, top: 2203\n",
      "40.14% completed and 99.069% cut.\n",
      "WW: 7428, VZ: 6050, top: 2218\n",
      "40.55% completed and 99.038% cut.\n",
      "WW: 7478, VZ: 6107, top: 2237\n",
      "40.96% completed and 99.013% cut.\n",
      "WW: 7526, VZ: 6150, top: 2264\n",
      "41.37% completed and 98.895% cut.\n",
      "WW: 7626, VZ: 6224, top: 2277\n",
      "41.78% completed and 98.914% cut.\n",
      "WW: 7708, VZ: 6281, top: 2312\n",
      "42.19% completed and 98.878% cut.\n",
      "WW: 7785, VZ: 6338, top: 2334\n",
      "42.60% completed and 98.945% cut.\n",
      "WW: 7862, VZ: 6390, top: 2350\n",
      "43.01% completed and 98.503% cut.\n",
      "WW: 7980, VZ: 6496, top: 2363\n",
      "43.42% completed and 98.757% cut.\n",
      "WW: 8064, VZ: 6570, top: 2377\n",
      "43.83% completed and 98.978% cut.\n",
      "WW: 8129, VZ: 6628, top: 2398\n",
      "44.24% completed and 98.847% cut.\n",
      "WW: 8219, VZ: 6697, top: 2422\n",
      "44.65% completed and 98.953% cut.\n",
      "WW: 8302, VZ: 6761, top: 2442\n",
      "45.06% completed and 99.095% cut.\n",
      "WW: 8350, VZ: 6803, top: 2465\n",
      "45.47% completed and 99.166% cut.\n",
      "WW: 8395, VZ: 6839, top: 2486\n",
      "45.88% completed and 99.079% cut.\n",
      "WW: 8434, VZ: 6878, top: 2515\n",
      "46.29% completed and 98.850% cut.\n",
      "WW: 8523, VZ: 6959, top: 2537\n",
      "46.70% completed and 98.913% cut.\n",
      "WW: 8599, VZ: 7029, top: 2561\n",
      "47.11% completed and 99.159% cut.\n",
      "WW: 8645, VZ: 7059, top: 2594\n",
      "47.52% completed and 99.024% cut.\n",
      "WW: 8715, VZ: 7109, top: 2613\n",
      "47.93% completed and 99.203% cut.\n",
      "WW: 8750, VZ: 7138, top: 2638\n",
      "48.34% completed and 98.942% cut.\n",
      "WW: 8807, VZ: 7184, top: 2663\n",
      "48.75% completed and 99.203% cut.\n",
      "WW: 8841, VZ: 7212, top: 2687\n",
      "49.16% completed and 98.761% cut.\n",
      "WW: 8930, VZ: 7302, top: 2703\n",
      "49.57% completed and 99.098% cut.\n",
      "WW: 8980, VZ: 7343, top: 2729\n",
      "49.98% completed and 98.756% cut.\n",
      "WW: 9081, VZ: 7429, top: 2746\n",
      "50.39% completed and 98.981% cut.\n",
      "WW: 9150, VZ: 7491, top: 2769\n",
      "50.80% completed and 99.114% cut.\n",
      "WW: 9202, VZ: 7536, top: 2789\n",
      "51.21% completed and 99.138% cut.\n",
      "WW: 9260, VZ: 7581, top: 2809\n",
      "51.61% completed and 99.188% cut.\n",
      "WW: 9300, VZ: 7614, top: 2828\n",
      "52.02% completed and 99.308% cut.\n",
      "WW: 9328, VZ: 7637, top: 2848\n",
      "52.43% completed and 99.323% cut.\n",
      "WW: 9355, VZ: 7657, top: 2874\n",
      "52.84% completed and 99.054% cut.\n",
      "WW: 9411, VZ: 7702, top: 2893\n",
      "53.25% completed and 98.875% cut.\n",
      "WW: 9491, VZ: 7785, top: 2914\n",
      "53.66% completed and 99.049% cut.\n",
      "WW: 9557, VZ: 7840, top: 2936\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "from math import sqrt\n",
    "isData = 1; \n",
    "\n",
    "if isData == 1: ds = data \n",
    "else: ds = background\n",
    "    \n",
    "columns = {\"lep_pt1\":[],\"lep_eta1\":[],\"lep_phi1\":[],\"lep_E1\":[], \"lep_m1\":[],\n",
    "           \"lep_pt2\":[],\"lep_eta2\":[],\"lep_phi2\":[],\"lep_E2\":[], \"lep_m2\":[],\n",
    "           \"jet_n\":[], \"jet_e\":[], \"jet_pt\": [], \"met\":[], \"weight\": [], \"label\": [], \"CR\" :[]}\n",
    "\n",
    "i = 0   \n",
    "count =0\n",
    "vz = 0\n",
    "ww = 0\n",
    "top = 0\n",
    "for event in ds: \n",
    "    \n",
    "    if i%100000 == 0 and i>0:\n",
    "        print(f\"{100*i/ds.GetEntries():.2f}% completed and {(100000-count)/1000:.3f}% cut.\")\n",
    "        print(f\"WW: {ww}, VZ: {vz}, top: {top}\")\n",
    "        count = 0\n",
    "    i += 1 \n",
    "    ## Data quality cuts: \n",
    "    \n",
    "    #if i > 100000: break\n",
    "    \n",
    "    #if ds.passGRL == 0: continue\n",
    "    #if ds.hasGoodVertex == 0: continue\n",
    "    if(ds.trigM == 0 and ds.trigE == 0):continue  \n",
    "\n",
    "    ## Event selection:\n",
    " \n",
    "    \n",
    "    ## Cut #1: Require (exactly) 2 leptons\n",
    "    if not (ds.lep_n == 2 or ds.lep_n == 3): continue\n",
    "    \n",
    "    ## Cut #2: Require opposite charge\n",
    "    if ds.lep_charge[0] == ds.lep_charge[1]: continue\n",
    "    \n",
    "    \n",
    "    if ds.lep_pt[0]/1000.0 < 25: continue\n",
    "    if ds.lep_etcone20[0]/ds.lep_pt[0] > 0.15: continue\n",
    "    if ds.lep_ptcone30[0]/ds.lep_pt[0] > 0.15: continue\n",
    "    #if not (ds.lep_flag[0] & 512): continue\n",
    "        \n",
    "    if ds.lep_pt[1]/1000.0 < 25: continue\n",
    "    if ds.lep_etcone20[1]/ds.lep_pt[1] > 0.15: continue\n",
    "    if ds.lep_ptcone30[1]/ds.lep_pt[1] > 0.15: continue\n",
    "    #if not (ds.lep_flag[1] & 512): continue\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Set Lorentz vectors: \n",
    "    l1.SetPtEtaPhiE(ds.lep_pt[0]/1000., ds.lep_eta[0], ds.lep_phi[0], ds.lep_E[0]/1000.);\n",
    "    l2.SetPtEtaPhiE(ds.lep_pt[1]/1000., ds.lep_eta[1], ds.lep_phi[1], ds.lep_E[1]/1000.);\n",
    "    ## Variables are stored in the TTree with unit MeV, so we need to divide by 1000 \n",
    "    ## to get GeV, which is a more practical and commonly used unit. \n",
    "    \n",
    "    \n",
    "    \n",
    "    e = 0\n",
    "    p = 0\n",
    "    nr_b = 0\n",
    "    for k in range(ds.jet_n):\n",
    "        e += ds.jet_E[k]\n",
    "        p += ds.jet_pt[k]\n",
    "        nr_b += ds.jet_MV2c10[k] > 0.85\n",
    "    \n",
    "        \n",
    "    \n",
    "    dileptons = l1 + l2;   \n",
    "    \n",
    "    #My cuts\n",
    "    if ds.met_et/1000.0 < 40: continue\n",
    "    if ds.lep_type[0] == ds.lep_type[1] and dileptons.M() < 121.2: continue\n",
    "    if nr_b > 1: continue\n",
    "    \n",
    "    #Cuts from 139f^-1 article. \n",
    "    #if ds.lep_type[0] == ds.lep_type[1] and dileptons.M() < 121.2: continue\n",
    "    #if dileptons.M()<100: continue\n",
    "    #if ds.met_et/1000.0 < 110: continue\n",
    "    met_sig = ds.met_et/(1000*sqrt(l1.Et() + l2.Et()))\n",
    "    #if met_sig < 10: continue \n",
    "    #if nr_b > 1: continue\n",
    "    \n",
    "    count += 1\n",
    "    \"\"\"\n",
    "    columns[\"lep_pt1\"].append(ds.lep_pt[0]/1000.0)\n",
    "    columns[\"lep_eta1\"].append(ds.lep_eta[0])\n",
    "    columns[\"lep_phi1\"].append(ds.lep_phi[0])\n",
    "    columns[\"lep_E1\"].append(ds.lep_E[0]/1000.0)\n",
    "    columns[\"lep_m1\"].append(l1.M())\n",
    "    \n",
    "    columns[\"lep_pt2\"].append(ds.lep_pt[1]/1000.0)\n",
    "    columns[\"lep_eta2\"].append(ds.lep_eta[1])\n",
    "    columns[\"lep_phi2\"].append(ds.lep_phi[1])\n",
    "    columns[\"lep_E2\"].append(ds.lep_E[1]/1000.0)\n",
    "    columns[\"lep_m2\"].append(l2.M())\n",
    "    \n",
    "    columns[\"met\"].append(ds.met_et/1000.0) \n",
    "    \n",
    "    columns[\"jet_n\"].append(ds.jet_n) \n",
    "    columns[\"jet_e\"].append(e/1000)\n",
    "    columns[\"jet_pt\"].append(p/1000)\n",
    "    \n",
    "    \n",
    "    if isData == 0:\n",
    "        columns[\"label\"].append(ds.channelNumber)\n",
    "        weight = ((ds.mcWeight)*(ds.scaleFactor_PILEUP)*\n",
    "                 (1)*(1)*\n",
    "                 (1))*((ds.XSection*lumi)/ds.SumWeights)\n",
    "        columns[\"weight\"].append(weight)\n",
    "    else:\n",
    "        columns[\"label\"].append(-999.0)\n",
    "        columns[\"weight\"].append(-999.0)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Deine the critical regions.\n",
    "    filled = False\n",
    "    if dileptons.Mt() > 60:\n",
    "        if ds.lep_type[0] != ds.lep_type[1]:\n",
    "            #if dileptons.Mt() < 65:\n",
    "            #if ds.met_et/1000 < 100 and ds.met_et/1000 > 60:\n",
    "                #if met_sig < 10 and met_sig > 10:\n",
    "            if dileptons.M() > 100:\n",
    "                if ds.jet_n ==0:\n",
    "                    #columns[\"CR\"].append(\"WW\")\n",
    "                    hist_WW_d.Fill(dileptons.M());\n",
    "                    filled = True\n",
    "                    ww += 1\n",
    "            #elif dileptons.Mt() > 80:\n",
    "            #elif ds.met_et/1000 > 110 and met_sig > 10:\n",
    "            #if dileptons.M() >100:\n",
    "                if nr_b == 1 and ds.jet_n ==1:\n",
    "                    #columns[\"CR\"].append(\"top\")\n",
    "                    hist_Top_d.Fill(dileptons.M());\n",
    "                    filled = True\n",
    "                    top += 1\n",
    "        #elif dileptons.Mt() > 120:\n",
    "        #if ds.met_et/1000>110:# and met_sig > 10:\n",
    "        #if dileptons.M() > 61.2 and dileptons.M()  < 121.2:\n",
    "        if abs(dileptons.M()-91.2) < 10:\n",
    "            if ds.jet_n ==0:\n",
    "                #columns[\"CR\"].append(\"VZ\")\n",
    "                hist_VZ_d.Fill(dileptons.M());\n",
    "                filled = True\n",
    "                vz += 1\n",
    "    #if not filled:\n",
    "        #columns[\"CR\"].append(\"other\")\n",
    "\n",
    "        \n",
    "print(\"Done!\")\n",
    "#df = pd.DataFrame(data=columns)\n",
    "#df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scale and classify the histograms (MC only) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are ready to make plots we need to do some further processing of the histograms we made above. The information necessary for doing the two steps below is found in the file **Infofile.txt**.   \n",
    "1. We need to **scale** the histograms to the right cross section and luminosity. Why? When making the MC samples a certain number of events is simulated, which will usually not correspond to the number of events in our data. The expected number of events from a certain kind of process is given by $N=\\sigma L$, where $\\sigma$ is the cross section and $L$ is the integrated luminosity. Therefore we need to scale each histogram by a scale factor <br> <br>\n",
    "$$sf = \\frac{N}{N_{MC}} = \\frac{ \\sigma L }{N_{MC}},$$ <br>  where $N_{MC}$ is the number of generated MC events.  <br> <br>\n",
    "2. We also need to **classify** the background processes into different categories. This is necessary when we eventually want to make the characteristic colorful background plots you might have seen before.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Make new histograms \n",
    "Maybe a bit depressingly we have to make a set of new histograms, this time corresponding to the different background categories, instead of the dataset IDs. Notice that these new histograms are made in a very similar way as above, i.e. with the same range and binning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_VZ = {}; H_WW = {}; H_Top = {}; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Backgrounds: \n",
    "    H_VZ[i] = R.TH1F() \n",
    "    H_WW[i] = R.TH1F() \n",
    "    H_Top[i] = R.TH1F() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Backgrounds: \n",
    "    H_VZ[i].SetNameTitle(\"hist_VZ\", \"Invariant mass, VZ\"); \n",
    "    H_WW[i].SetNameTitle(\"hist_WW\", \"Invariant mass, WW\"); \n",
    "    H_Top[i].SetNameTitle(\"hist_Top\", \"Invariant mass, Top\");\n",
    "    H_VZ[i].SetBins(20,0,300); \n",
    "    H_WW[i].SetBins(20,0,300);\n",
    "    H_Top[i].SetBins(20,0,300); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Scale and add histograms \n",
    "Now we read our info file, scale all (old) histograms, and then add them to the new histograms we just defined.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Backgrounds: \n",
    "    H_VZ[i].Reset(); \n",
    "    #H_WW[i].Reset(); \n",
    "    #H_Top[i].Reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dsid in hist_VZ.keys(): \n",
    "    \n",
    "    Type = MCcat[dsid]\n",
    "    print(Type)\n",
    "    H_VZ[Type].Add(hist_VZ[dsid]); \n",
    "    #H_WW[Type].Add(hist_WW[dsid]); \n",
    "    #H_Top[Type].Add(hist_Top[dsid]); \n",
    "\n",
    "infofile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Color the histograms \n",
    "Make yet another map, this time containing the colors you want the backgrounds to have, and then set the colors of your histograms. Note that colors are defined by integers in ROOT. If you are not happy with the colors chosen below you can have look at the [TColor](https://root.cern.ch/doc/master/classTColor.html) class reference for more options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors[\"Diboson\"] = R.kGreen; \n",
    "colors[\"Zjets\"] = R.kYellow; \n",
    "colors[\"ttbar\"] = R.kRed;\n",
    "colors[\"singleTop\"] = R.kBlue-7; \n",
    "colors[\"Wjets\"] = R.kBlue+3; \n",
    "colors[\"topX\"] = R.kOrange+1; \n",
    "colors[\"Higgs\"] = R.kMagenta; \n",
    "colors[\"Wjetsincl\"] = R.kBlue-10;\n",
    "colors[\"Zjetsincl\"] = R.kYellow-9;\n",
    "colors[\"SUSYSlepSlep\"] = R.kYellow-9;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in Backgrounds: \n",
    "    H_VZ[h].SetFillColor(colors[h]); \n",
    "    H_Top[h].SetFillColor(colors[h]);\n",
    "    H_WW[h].SetFillColor(colors[h]);\n",
    "    \n",
    "    H_VZ[h].SetLineColor(colors[h]); \n",
    "    H_Top[h].SetLineColor(colors[h]);\n",
    "    H_WW[h].SetLineColor(colors[h]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stack and plot the histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have arrived to the part where we can plot the results of all the work done above. For each variable we need to stack the backgrounds on top of each other, which is done by using the [THStack](https://root.cern.ch/doc/master/classTHStack.html) class. In the example below we do this for two variables; invariant mass and missing $E_T$.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_VZ = R.THStack(\"Invariant mass\", \"\");\n",
    "stack_Top = R.THStack(\"Missing ET\", \"\"); \n",
    "stack_WW = R.THStack(\"Lepton pT\", \"\"); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in Backgrounds: \n",
    "    stack_VZ.RecursiveRemove(H_VZ[h]); ## Remove previously stacked histograms  \n",
    "    stack_Top.RecursiveRemove(H_Top[h]);\n",
    "    stack_WW.RecursiveRemove(H_WW[h]);\n",
    "    stack_VZ.Add(H_VZ[h]); \n",
    "    stack_Top.Add(H_Top[h]);\n",
    "    stack_WW.Add(H_WW[h]); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make a legend (see [TLegend](https://root.cern.ch/doc/master/classTLegend.html)), and add  the different backgrounds. Next we make a canvas (see [TCanvas](https://root.cern.ch/doc/master/classTCanvas.html)), which is allways necessary when we want to make a plot. Then you draw the stack and the legend, and display them by drawing the canvas. We can also specify axis labels and a bunch of other stuff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R.gStyle.SetLegendBorderSize(0); ## Remove (default) border around legend \n",
    "leg = R.TLegend(0.65, 0.60, 0.9, 0.85); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leg.Clear();\n",
    "for i in Backgrounds: \n",
    "    leg.AddEntry(H_VZ[i], i, \"f\")  ## Add your histograms to the legend\n",
    "leg.AddEntry(hist_VZ_d, \"Data\", \"lep\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = R.TCanvas(\"c\", \"c\", 600, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R.gPad.SetLogy() ## Set logarithmic y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_VZ_d.SetLineColor(R.kBlack); \n",
    "hist_VZ_d.SetMarkerStyle(R.kFullCircle); \n",
    "hist_VZ_d.SetMarkerColor(R.kBlack); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_VZ.Draw(\"hist\"); \n",
    "stack_VZ.SetMaximum(1E6); \n",
    "stack_VZ.SetMinimum(1); \n",
    "stack_VZ.GetYaxis().SetTitle(\"# events\");\n",
    "stack_VZ.GetYaxis().SetTitleOffset(1.3); \n",
    "stack_VZ.GetXaxis().SetTitle(\"VZ - m_{ll} (GeV)\");\n",
    "stack_VZ.GetXaxis().SetTitleOffset(1.3);\n",
    "hist_VZ_d.Draw(\"same E\"); \n",
    "leg.Draw();\n",
    "C.Draw();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_Top_d.SetLineColor(R.kBlack); \n",
    "hist_Top_d.SetMarkerStyle(R.kFullCircle); \n",
    "hist_Top_d.SetMarkerColor(R.kBlack); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_Top.Draw(\"hist\"); \n",
    "stack_Top.SetMaximum(1E6); \n",
    "stack_Top.GetYaxis().SetTitle(\"# events\");\n",
    "stack_Top.GetYaxis().SetTitleOffset(1.3); \n",
    "stack_Top.GetXaxis().SetTitle(\"Top - m_{ll} (GeV)\");\n",
    "stack_Top.GetXaxis().SetTitleOffset(1.3);\n",
    "hist_Top_d.Draw(\"same e\"); \n",
    "leg.Draw();\n",
    "C.Draw(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_WW_d.SetLineColor(R.kBlack); \n",
    "hist_WW_d.SetMarkerStyle(R.kFullCircle); \n",
    "hist_WW_d.SetMarkerColor(R.kBlack); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_WW.Draw(\"hist\"); \n",
    "stack_WW.SetMaximum(1E6); \n",
    "stack_WW.GetYaxis().SetTitle(\"# events\");\n",
    "stack_WW.GetYaxis().SetTitleOffset(1.3); \n",
    "stack_WW.GetXaxis().SetTitle(\"WW - m_{ll} (GeV)\");\n",
    "stack_WW.GetXaxis().SetTitleOffset(1.3);\n",
    "hist_WW_d.Draw(\"same e\"); \n",
    "leg.Draw();\n",
    "C.Draw(); "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
